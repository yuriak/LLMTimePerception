{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517eb211-8936-4ef4-a796-6c83f65a8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6923d1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm.LLM at 0x1555169473d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from llm import LLM\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "llm = LLM(Namespace(inference_mode=\"api_async\", api_key=\"None\", base_url=\"http://node16:9876/v1/\", llm_in_use=\"meta-llama/Llama-3.3-70B-Instruct\", fast_mode=False, max_retry=3, max_tokens=16384, num_workers=1, max_batch_size=16))\n",
    "\n",
    "llm.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82634d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4712a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = glob.glob(\"../tpqa_result_final_merge/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d797eb-ae04-4323-8459-d3a40235c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {}\n",
    "for x in all_results:\n",
    "    model_name, dataset_name = x.split(\"/\")[-2], x.split(\"/\")[-1]\n",
    "    model_name = model_name.split(\"_\")[-1]\n",
    "    dataset_name = dataset_name.replace(\"_results.json\",\"\")\n",
    "    if model_name not in final_results:\n",
    "        final_results[model_name] = {}\n",
    "    if dataset_name not in final_results[model_name]:\n",
    "        final_results[model_name][dataset_name] = json.load(open(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dcccca-64f0-48df-955b-d9f623614170",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    'Llama-3.1-8B-Instruct',\n",
    "    'Qwen2.5-7B-Instruct',\n",
    "    'Llama-3.3-70B-Instruct',\n",
    "    'Qwen2.5-72B-Instruct',\n",
    "    'DeepSeek-R1-Distill-Llama-70B',\n",
    "    'QwQ-32B'\n",
    "]\n",
    "\n",
    "all_tasks = [\n",
    "    'type1_dataset', \n",
    "    'type1_easy_dataset',\n",
    "    'type1_very_easy_dataset', \n",
    "    'type2_dataset',\n",
    "    'type2_misleading_dataset',\n",
    "    'type2_misleading_with_token_dataset',\n",
    "]\n",
    "\n",
    "sample_model = all_models[0]\n",
    "sample_task = all_tasks[0]\n",
    "\n",
    "import numpy as np\n",
    "# final_results[sample_model][sample_task]['runs'][0].keys()\n",
    "# dict_keys(['accuracy', 'total_count', 'correct_count', 'results'])\n",
    "\n",
    "sample_avg_accuracy = np.mean([ x['accuracy'] for x in final_results[sample_model][sample_task]['runs']])\n",
    "\n",
    "import pandas as pd\n",
    "# Create a Table: compute average accuracy for all of the combination in all_models and all_tasks, put them into a pandas table and assigned to the variable df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1977c610-09d4-4d3e-aaac-d2f4969fbe79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'total_count', 'correct_count', 'results'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[sample_model][sample_task]['runs'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85973dcc-dcd1-45a4-bf22-727aa397201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {model: {} for model in all_models}\n",
    "\n",
    "# Iterate over all models and tasks\n",
    "for model in all_models:\n",
    "    for task in all_tasks:\n",
    "        # Compute the average accuracy for the current model-task pair\n",
    "        avg_accuracy = np.mean([x['accuracy'] for x in final_results[model][task]['runs']])\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        results[model][task] = avg_accuracy\n",
    "\n",
    "# Create a pandas DataFrame from the results\n",
    "df = pd.DataFrame(results).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36662ff1-9885-4ed3-927a-526580036d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36f19b6-2e32-4b15-b7b3-78eebda6bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a34a51b-cd42-4bce-9c94-fd0559f550dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a3a6c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a3a6c_level0_col0\" class=\"col_heading level0 col0\" >type1_dataset</th>\n",
       "      <th id=\"T_a3a6c_level0_col1\" class=\"col_heading level0 col1\" >type1_easy_dataset</th>\n",
       "      <th id=\"T_a3a6c_level0_col2\" class=\"col_heading level0 col2\" >type1_very_easy_dataset</th>\n",
       "      <th id=\"T_a3a6c_level0_col3\" class=\"col_heading level0 col3\" >type2_dataset</th>\n",
       "      <th id=\"T_a3a6c_level0_col4\" class=\"col_heading level0 col4\" >type2_misleading_dataset</th>\n",
       "      <th id=\"T_a3a6c_level0_col5\" class=\"col_heading level0 col5\" >type2_misleading_with_token_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row0\" class=\"row_heading level0 row0\" >Llama-3.1-8B-Instruct</th>\n",
       "      <td id=\"T_a3a6c_row0_col0\" class=\"data row0 col0\" >79.73</td>\n",
       "      <td id=\"T_a3a6c_row0_col1\" class=\"data row0 col1\" >83.00</td>\n",
       "      <td id=\"T_a3a6c_row0_col2\" class=\"data row0 col2\" >99.40</td>\n",
       "      <td id=\"T_a3a6c_row0_col3\" class=\"data row0 col3\" >76.80</td>\n",
       "      <td id=\"T_a3a6c_row0_col4\" class=\"data row0 col4\" >47.73</td>\n",
       "      <td id=\"T_a3a6c_row0_col5\" class=\"data row0 col5\" >16.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row1\" class=\"row_heading level0 row1\" >Qwen2.5-7B-Instruct</th>\n",
       "      <td id=\"T_a3a6c_row1_col0\" class=\"data row1 col0\" >66.80</td>\n",
       "      <td id=\"T_a3a6c_row1_col1\" class=\"data row1 col1\" >71.20</td>\n",
       "      <td id=\"T_a3a6c_row1_col2\" class=\"data row1 col2\" >92.67</td>\n",
       "      <td id=\"T_a3a6c_row1_col3\" class=\"data row1 col3\" >80.93</td>\n",
       "      <td id=\"T_a3a6c_row1_col4\" class=\"data row1 col4\" >26.80</td>\n",
       "      <td id=\"T_a3a6c_row1_col5\" class=\"data row1 col5\" >12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row2\" class=\"row_heading level0 row2\" >Llama-3.3-70B-Instruct</th>\n",
       "      <td id=\"T_a3a6c_row2_col0\" class=\"data row2 col0\" >91.33</td>\n",
       "      <td id=\"T_a3a6c_row2_col1\" class=\"data row2 col1\" >93.20</td>\n",
       "      <td id=\"T_a3a6c_row2_col2\" class=\"data row2 col2\" >99.87</td>\n",
       "      <td id=\"T_a3a6c_row2_col3\" class=\"data row2 col3\" >93.47</td>\n",
       "      <td id=\"T_a3a6c_row2_col4\" class=\"data row2 col4\" >74.00</td>\n",
       "      <td id=\"T_a3a6c_row2_col5\" class=\"data row2 col5\" >40.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row3\" class=\"row_heading level0 row3\" >Qwen2.5-72B-Instruct</th>\n",
       "      <td id=\"T_a3a6c_row3_col0\" class=\"data row3 col0\" >83.80</td>\n",
       "      <td id=\"T_a3a6c_row3_col1\" class=\"data row3 col1\" >84.53</td>\n",
       "      <td id=\"T_a3a6c_row3_col2\" class=\"data row3 col2\" >99.93</td>\n",
       "      <td id=\"T_a3a6c_row3_col3\" class=\"data row3 col3\" >96.60</td>\n",
       "      <td id=\"T_a3a6c_row3_col4\" class=\"data row3 col4\" >82.53</td>\n",
       "      <td id=\"T_a3a6c_row3_col5\" class=\"data row3 col5\" >48.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row4\" class=\"row_heading level0 row4\" >DeepSeek-R1-Distill-Llama-70B</th>\n",
       "      <td id=\"T_a3a6c_row4_col0\" class=\"data row4 col0\" >87.93</td>\n",
       "      <td id=\"T_a3a6c_row4_col1\" class=\"data row4 col1\" >92.27</td>\n",
       "      <td id=\"T_a3a6c_row4_col2\" class=\"data row4 col2\" >100.00</td>\n",
       "      <td id=\"T_a3a6c_row4_col3\" class=\"data row4 col3\" >99.20</td>\n",
       "      <td id=\"T_a3a6c_row4_col4\" class=\"data row4 col4\" >98.00</td>\n",
       "      <td id=\"T_a3a6c_row4_col5\" class=\"data row4 col5\" >94.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3a6c_level0_row5\" class=\"row_heading level0 row5\" >QwQ-32B</th>\n",
       "      <td id=\"T_a3a6c_row5_col0\" class=\"data row5 col0\" >91.53</td>\n",
       "      <td id=\"T_a3a6c_row5_col1\" class=\"data row5 col1\" >94.80</td>\n",
       "      <td id=\"T_a3a6c_row5_col2\" class=\"data row5 col2\" >100.00</td>\n",
       "      <td id=\"T_a3a6c_row5_col3\" class=\"data row5 col3\" >99.33</td>\n",
       "      <td id=\"T_a3a6c_row5_col4\" class=\"data row5 col4\" >99.20</td>\n",
       "      <td id=\"T_a3a6c_row5_col5\" class=\"data row5 col5\" >99.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1554170c72e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890d613b-8891-4ea8-9af8-b11f5c129fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43b5a08-5591-46e6-9f08-81ba01582b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "# creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
    "# client = gspread.authorize(creds)\n",
    "\n",
    "# spreadsheet = client.open(\"TimeEvalResults\")\n",
    "\n",
    "# spreadsheet.worksheets()\n",
    "\n",
    "# working_sheet = spreadsheet.worksheet(\"TEST\")\n",
    "\n",
    "# set_with_dataframe(working_sheet, df, include_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e3a80-5cc7-4e19-8bd0-102ba6ec9269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a3fe63-3dc9-4703-bfac-054deee125da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f97862f3-0658-4a40-9ac0-9d794403d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are a classification agent. Your task is to classify the reasoning provided by an LLM into one of the following four categories, based on **how** the LLM determines which response took longer time to generate:\n",
    "\n",
    "### Allowed categories (return only one of the category names):\n",
    "- `time`\n",
    "- `text_length`\n",
    "- `semantic`\n",
    "- `other`\n",
    "\n",
    "### Classification Rules:\n",
    "\n",
    "1. **`time`**:  \n",
    "   The reason explicitly involves **timing information** — such as start time, end time, duration (e.g., “1 minute and 45 seconds”), timestamps, or calculations of elapsed time.  \n",
    "   If the decision is made **primarily or solely based on these time-based values**, without switching judgment due to other factors, classify it as `time`.\n",
    "\n",
    "2. **`text_length`**:  \n",
    "   The reason makes a judgment based on the **length of the text**, such as token count, number of words, number of sentences, or how long the generated response is.  \n",
    "   This includes explicitly mentioning phrases like “Response A is longer,” “has more tokens,” or “took more space to explain”, etc.\n",
    "\n",
    "3. **`semantic`**:  \n",
    "   The reason does **not mention time or length difference** at all, but solely relies on **semantic or cognitive complexity** — such as the depth of explanation, difficulty of the topic, use of logic or math, or other indicators of **conceptual effort**.\n",
    "\n",
    "4. **`other`**:  \n",
    "   Use this category if the reasoning doesn’t clearly match any of the above — for example, if the model relies on **irrelevant metadata**, contradictory logic, unclear rationale, or vague comparison that doesn’t fit well into the previous categories.\n",
    "\n",
    "Do **not** include any explanation or justification in your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32380676-0d79-4601-b3e4-1b5b0fcc5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "\n",
    "Here is the explanation to classify:\n",
    "```\n",
    "{reason}\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61dd021e-d08b-41a9-91d4-3d00073b4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    'Llama-3.1-8B-Instruct',\n",
    "    'Qwen2.5-7B-Instruct',\n",
    "    'Llama-3.3-70B-Instruct',\n",
    "    'Qwen2.5-72B-Instruct',\n",
    "    'DeepSeek-R1-Distill-Llama-70B',\n",
    "    'QwQ-32B'\n",
    "]\n",
    "\n",
    "all_tasks = [\n",
    "    'type1_dataset', \n",
    "    'type1_easy_dataset',\n",
    "    'type1_very_easy_dataset', \n",
    "    'type2_dataset',\n",
    "    'type2_misleading_dataset',\n",
    "    'type2_misleading_with_token_dataset',\n",
    "]\n",
    "\n",
    "sample_model = all_models[0]\n",
    "sample_task = all_tasks[0]\n",
    "\n",
    "import numpy as np\n",
    "# final_results[sample_model][sample_task]['runs'][0].keys()\n",
    "# dict_keys(['accuracy', 'total_count', 'correct_count', 'results'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b709b6c3-28f0-4498-98fa-5ae57e3e4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_response(text):\n",
    "    return text.replace(\"<think>\", \"\").replace(\"</think>\",\"\").replace(\"<solution>\", \"\").replace(\"</solution>\",\"\")\n",
    "def get_reason(result):\n",
    "    if result['parsed_response'] is not None and \"reason\" in result['parsed_response']:\n",
    "        return result['parsed_response']['reason']\n",
    "    return get_raw_response(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6e22f9-f79d-46d4-8661-db470fb3e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promtpify(reason):\n",
    "    if len(reason) > 10000:\n",
    "        reason = reason[-10000:]\n",
    "    return [{\n",
    "        \"role\":\"system\",\n",
    "        \"content\": sys_prompt.strip()\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt.format(reason=reason).strip()\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a82432-7c1b-4cc6-8fee-d028b8d4b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eb_args = {\"extra_body\": {\"guided_choice\": [\"time\", \"text_length\", \"semantic\", \"other\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bf724f2-a82e-4dbb-9fd2-33a4549dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_prompts= []\n",
    "# for sample_model in all_models:\n",
    "#     for sample_task in all_tasks:\n",
    "#         for run in final_results[sample_model][sample_task]['runs']:\n",
    "#             # print(f\"Running {sample_model} {sample_task}\")\n",
    "#             results = run['results']\n",
    "#             reason_prompt = [ promtpify(get_reason(x)) for x in results]\n",
    "#             all_prompts.extend(reason_prompt)\n",
    "#             # responses = llm.generate(reason_prompt, **eb_args)\n",
    "#             # attributions = list(map(lambda x: x['solution'], responses['responses']))\n",
    "#             # assert len(attributions) == len(reason_prompt[0])\n",
    "#             # for i, result in enumerate(run['results']):\n",
    "#             #     run['results'][i]['attribute'] = attributions[i]\n",
    "\n",
    "# # json.dump(all_prompts, open(\"./batch_prompt.json\", 'w'))\n",
    "\n",
    "# # all_prompts[0]\n",
    "\n",
    "# # all_responses = llm.generate(all_prompts, **eb_args)\n",
    "\n",
    "\n",
    "\n",
    "# len(all_prompts)\n",
    "\n",
    "# len(all_prompts)\n",
    "\n",
    "# saved_files = []\n",
    "# for i in range(len(all_prompts)):\n",
    "#     saved_files.append({\"custom_id\": f\"request-{i}\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"meta-llama/Meta-Llama-3.3-70B-Instruct\", \"messages\": all_prompts[i], \"max_completion_tokens\": 1000, \"extra_body\": {\"guided_choice\": [\"time\", \"text_length\", \"semantic\", \"other\"]}}})\n",
    "\n",
    "# open(\"./batch_file.jsonl\",'w').write(\"\\n\".join([json.dumps(x) for x in saved_files])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ee4245-76ab-4659-9988-0b51d14f4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = json.load(open(\"./batch_prompt_output.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83d402a-a8f1-480e-826e-c1d98b7056ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributions = list(map(lambda x:x['solution'], responses['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db99a7fe-bd7c-49d1-b6c1-b1ef17acc528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample_model in all_models:\n",
    "    for sample_task in all_tasks:\n",
    "        for run in final_results[sample_model][sample_task]['runs']:\n",
    "            # print(f\"Running {sample_model} {sample_task}\")\n",
    "            results = run['results']\n",
    "            # responses = llm.generate(reason_prompt, **eb_args)\n",
    "            # attributions = list(map(lambda x: x['solution'], responses['responses']))\n",
    "            # assert len(attributions) == len(reason_prompt[0])\n",
    "            for i, result in enumerate(run['results']):\n",
    "                run['results'][i]['attribute'] = attributions.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1a3ea8c-9bd8-4aec-99b9-fe9ddcc8ebea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QwQ-32B'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "455495ec-9bcc-477b-b6c3-170481b52a33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'type2_misleading_with_token_dataset'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da367bd5-6581-4e51-a50f-a9a5e2f37067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec8b0885-4632-4424-8d4a-4267e7c1a10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for sample_model in all_models:\n",
    "    for sample_task in all_tasks:\n",
    "        attr_result = pd.concat([pd.DataFrame(final_results[sample_model][sample_task]['runs'][i]['results'])['attribute'].value_counts() for i in range(5)],axis=1).fillna(0).mean(1)\n",
    "        rows.append({\"model\": sample_model, \"task\": sample_task, **attr_result.to_dict()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91ed0680-c13d-42cb-a8ce-7de3202ea5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attr_statistics = pd.DataFrame(rows).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "666dbde0-6fec-43fb-8951-d46cb8a80707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "# creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
    "# client = gspread.authorize(creds)\n",
    "\n",
    "# spreadsheet = client.open(\"TimeEvalResults\")\n",
    "\n",
    "# spreadsheet.worksheets()\n",
    "\n",
    "\n",
    "# spreadsheet.add_worksheet(\"AttributionStatistics\", rows=attr_statistics.shape[0]+1, cols=attr_statistics.shape[1]+1)\n",
    "\n",
    "\n",
    "# working_sheet = spreadsheet.worksheet(\"AttributionStatistics\")\n",
    "\n",
    "# set_with_dataframe(working_sheet, attr_statistics, include_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9a178199-5fcd-4245-9fbd-1e6f40cdf519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_attrs = [\"time\", \"text_length\", \"semantic\", \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "45605f36-c2dd-4b6a-8677-7dc4ee060d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for sample_model in all_models:\n",
    "    for sample_task in all_tasks:\n",
    "        correct_ratio = {}\n",
    "        for attr in all_attrs:\n",
    "            attr_correct = 0\n",
    "            attr_incorrect = 0\n",
    "            attr_count = 0\n",
    "            for run in final_results[sample_model][sample_task]['runs']:\n",
    "                for result in run['results']:\n",
    "                    if result['attribute'] == attr:\n",
    "                        attr_count +=1\n",
    "                        if result['correct']:\n",
    "                            attr_correct +=1\n",
    "                        else:\n",
    "                            attr_incorrect +=1\n",
    "            num_runs = len(final_results[sample_model][sample_task]['runs'])\n",
    "            attr_correct = attr_correct / num_runs\n",
    "            attr_incorrect = attr_incorrect / num_runs\n",
    "            attr_count = attr_count / num_runs\n",
    "            # correct_ratio[attr] = (attr_correct / attr_count if attr_count > 0 else 0.0 ) * 100\n",
    "            correct_ratio[attr] = attr_correct\n",
    "            \n",
    "        rows.append({\"model\": sample_model, \"task\": sample_task, **correct_ratio})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "884d559e-93cc-4498-9d8a-92816f21b869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attr_corr_statistics = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cadf1455-1df4-410d-9e07-ae56524f1eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "spreadsheet = client.open(\"TimeEvalResults\")\n",
    "\n",
    "spreadsheet.worksheets()\n",
    "\n",
    "\n",
    "spreadsheet.add_worksheet(\"AttributionCorrectStatisticsBAR\", rows=attr_corr_statistics.shape[0]+1, cols=attr_corr_statistics.shape[1]+1)\n",
    "\n",
    "\n",
    "working_sheet = spreadsheet.worksheet(\"AttributionCorrectStatisticsBAR\")\n",
    "\n",
    "set_with_dataframe(working_sheet, attr_corr_statistics, include_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23a3db26-e16e-495c-9ec4-f4d436280cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2c468a6-2af6-4a96-98fb-2773f6cd1272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://node16:9876/metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
