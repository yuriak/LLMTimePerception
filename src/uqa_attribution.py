import sys
import json
import argparse
import tiktoken
from llm import LLM
from argparse import Namespace
from tqdm import tqdm
from utils import extract_json_from_text, validate_and_parse_json_output

tokenizer = tiktoken.encoding_for_model("gpt-4o")

SYSTEM_PROMPT = """
You are an expert in analyzing and comparing reasoning structures in natural language responses. You will be provided with two responses (**Response A** and **Response B**) to the **same question**. These responses are generated by an AI system under different conditions, but you are **not** told what the conditions are. You are expected to **objectively analyze and annotate** the structure and style of the responses, without assuming which is preferred or better. You don't need to consider the correctness of the responses.

You are also informed that in the original question, the multiple-choice options (e.g., A/B/C/D) may have appeared in **different orders**. Therefore, even if the final answer labels (e.g., `"answer": "B"`) differ between A and B, this **does not indicate a mistake**. You should **not** treat differences in answer label letters as meaningful. Instead, focus entirely on the **reasoning path** and **language structure** used to reach the conclusion.

You will be given the **token count** of each response. Focus solely on **what is expressed and how it is structured**, not on performance or correctness.

---

### Instructions:

You must analyze the presence and expression of each of the following **ten reasoning and presentation components**. For each component, annotate in the following JSON format:

```json
{
  "<component_name>": {
    "A_has": true/false,
    "B_has": true/false,
    "B_compare_to_A": "simplified" / "same" / "complexified"
  }
}
```

At the end of the dictionary, include two **overall** comparison fields:

* `"overall_verbalization"`: comparison of **language style** and surface expression in B relative to A
* `"overall_reasoning_logic"`: comparison of **reasoning steps and structure** in B relative to A

Only use these comparison values:

* `"simplified"`: B is more concise, skips elements, or is less elaborate than A.
* `"same"`: B and A are similar in this aspect.
* `"complexified"`: B introduces additional detail, steps, or structure beyond A.

---

### Components and Annotation Criteria:

1. **task\_restatement**

   * **Definition**: Whether the response explicitly rephrases or summarizes the task before beginning the solution.
   * **A\_has / B\_has = true**: Phrases like “We are asked to…”, “To find the value of…”, “The problem requires us to…”.
   * **= false**: The response immediately begins with solution steps, formulas, or definitions without restating the task.
   * **B_compare_to_A**:

     * `"simplified"`: B omits or shortens the restatement found in A.
     * `"same"`: Both A and B provide similarly scoped restatements.
     * `"complexified"`: B adds additional framing, reformulation, or interpretations compared to A.

2. **task\_decomposition**

   * **Definition**: Whether the response explicitly breaks the task into smaller steps or subtasks.
   * **A\_has / B\_has = true**: Presence of step numbers (e.g., “Step 1: …”), bullet points, or clear transitions like “First,… Then,… Finally,…”.
   * **= false**: The response proceeds linearly without explicit decomposition.
   * **B_compare_to_A**:

     * `"simplified"`: B merges multiple subtasks into a single paragraph or reduces step markers.
     * `"same"`: Both responses decompose the task similarly.
     * `"complexified"`: B introduces more granularity or new step-wise segmentation compared to A.

3. **formalization**

   * **Definition**: Whether the response translates parts of the reasoning into mathematical or logical notation.
   * **A\_has / B\_has = true**: Use of equations, symbols (like ∀, ∑, √), matrix/vector notation, or formal logic.
   * **= false**: Purely verbal explanation without any symbolic/formal elements.
   * **B_compare_to_A**:

     * `"simplified"`: B replaces formal expressions with plain language or omits equations.
     * `"same"`: Both use similar levels of formalism.
     * `"complexified"`: B adds additional equations or expresses steps more rigorously using symbolic form.

4. **math\_reasoning**

   * **Definition**: Whether the response includes step-by-step mathematical computation, transformation, or algebraic manipulation.
   * **A\_has / B\_has = true**: Includes arithmetic operations, equation solving, algebraic simplification, matrix multiplication, etc.
   * **= false**: Mentions results or conclusions without showing intermediate math steps.
   * **B_compare_to_A**:

     * `"simplified"`: B skips some steps or condenses multiple operations into one.
     * `"same"`: Step-by-step reasoning is comparable in both.
     * `"complexified"`: B includes additional breakdowns, explanations, or detailed calculations.

5. **intermediate\_reasoning**

   * **Definition**: Whether the response explicitly explains how intermediate steps lead to the next.
   * **A\_has / B\_has = true**: Transitions such as “therefore”, “which means that”, “thus we can conclude…”, or commentary between math steps.
   * **= false**: Steps are listed without connective reasoning or bridging logic.
   * **B_compare_to_A**:

     * `"simplified"`: B drops explanatory links found in A.
     * `"same"`: Both maintain similar logical continuity.
     * `"complexified"`: B elaborates on intermediate justifications or makes previously implicit logic explicit.

6. **correction**

   * **Definition**: Whether the response includes an explicit correction of a prior step, clarification of a mistake, or alternate consideration.
   * **A\_has / B\_has = true**: Phrases like “Actually, that’s incorrect…”, “Let’s correct this…”, “This doesn’t hold, so…”.
   * **= false**: The reasoning is linear and does not revisit or revise previous steps.
   * **B_compare_to_A**:

     * `"simplified"`: B removes a correction present in A.
     * `"same"`: Both have similar correction behavior (or both absent).
     * `"complexified"`: B adds new correction or revision steps not in A.

7. **reflection\_or\_hypothesis**

   * **Definition**: Whether the response includes meta-reasoning or speculative reasoning beyond direct steps.
   * **A\_has / B\_has = true**: Includes guesses, hypothetical scenarios, or broader interpretation like “It’s reasonable to assume…”, “This suggests that…”.
   * **= false**: The response sticks strictly to procedural steps without commentary.
   * **B_compare_to_A**:

     * `"simplified"`: B omits speculative or interpretive parts from A.
     * `"same"`: Both have equivalent reflection content.
     * `"complexified"`: B adds reasoning that goes beyond the immediate problem-solving (e.g., discussing implications).

8. **illustration**

   * **Definition**: Whether the response uses analogies, side examples, or concrete illustrations to clarify ideas.
   * **A\_has / B\_has = true**: Statements like “For example…”, hypothetical samples, or metaphorical comparisons.
   * **= false**: No illustrative aids; solution is fully abstract or literal.
   * **B_compare_to_A**:

     * `"simplified"`: B removes such illustrations.
     * `"same"`: Illustrative content is comparable.
     * `"complexified"`: B introduces new analogies or examples to enhance clarity.

9. **answer\_justification**

   * **Definition**: Whether the final answer is supported with a verbal or mathematical justification.
   * **A\_has / B\_has = true**: Includes reasoning like “This is because…”, or final numerical substitution to verify.
   * **= false**: The answer is provided without further explanation.
   * **B_compare_to_A**:

     * `"simplified"`: B reduces explanation around the answer.
     * `"same"`: Justification level is equivalent.
     * `"complexified"`: B adds verbal reasoning or more detailed verification.

10. **answer\_presentation**

    * **Definition**: How the final answer is presented in terms of formatting and visibility.
    * **A\_has / B\_has = true**: Final answer is boxed, quoted, labeled with “Final answer:”, or appears in a structured JSON object.
    * **= false**: The answer is embedded in a paragraph without formatting or clear marking.
    * **B_compare_to_A**:

      * `"simplified"`: B presents the answer with less formatting.
      * `"same"`: Answer formatting is the same.
      * `"complexified"`: B uses clearer formatting or more emphatic presentation (e.g., more visual separation).

---

### Final Comparison Fields:

```json
"overall_verbalization": "simplified" / "same" / "complexified"
```

* **Compares** surface-level language expression: compactness, formatting, sentence structure, use of symbols, whitespace.

```json
"overall_reasoning_logic": "simplified" / "same" / "complexified"
```

* **Compares** the logical path: number of steps, structure of reasoning, use of assumptions or derivations.

---

### Output Format

```json
{
  "task_restatement": {
    "A_has": true,
    "B_has": false,
    "B_compare_to_A": "simplified"
  },
  "task_decomposition": {
    "A_has": true,
    "B_has": true,
    "B_compare_to_A": "same"
  },
  "formalization": {
    "A_has": true,
    "B_has": true,
    "B_compare_to_A": "same"
  },
  "math_reasoning": {
    "A_has": true,
    "B_has": true,
    "B_compare_to_A": "same"
  },
  "answer_presentation": {
    "A_has": true,
    "B_has": false,
    "B_compare_to_A": "simplified"
  },
  "intermediate_reasoning": {
    "A_has": true,
    "B_has": true,
    "B_compare_to_A": "simplified"
  },
  "correction": {
    "A_has": false,
    "B_has": false,
    "B_compare_to_A": "same"
  },
  "illustration": {
    "A_has": false,
    "B_has": false,
    "B_compare_to_A": "same"
  },
  "answer_justification": {
    "A_has": true,
    "B_has": true,
    "B_compare_to_A": "same"
  },
  "reflection_or_hypothesis": {
    "A_has": false,
    "B_has": false,
    "B_compare_to_A": "same"
  },
  "overall_verbalization": "simplified",
  "overall_reasoning_logic": "same"
}
```

You must return a single JSON object in the format above. **Do not include any explanation, text, or comments** before or after the JSON.
"""

USER_PROMPT = """
### Response A
{response_a}

### Number of tokens in Response A
{num_tokens_a}

### Response B
{response_b}

### Number of tokens in Response B
{num_tokens_b}
"""

parser = argparse.ArgumentParser(description="Run UQA attribution")
parser.add_argument(
    "--input_file_a",
    type=str,
    help="Path to the input file containing question and response A",
    required=True,
)
parser.add_argument(
    "--input_file_b",
    type=str,
    help="Path to the input file containing question and response B",
    required=True,
)
parser.add_argument(
    "--output_file",
    type=str,
    help="Path to the output file to save annotations",
    required=True,
)

args = parser.parse_args()
input_file_a = args.input_file_a
input_file_b = args.input_file_b
output_file = args.output_file

with open(input_file_a, "r") as f:
    data_a = json.load(f)

with open(input_file_b, "r") as f:
    data_b = json.load(f)

assert len(data_a['runs']) == len(data_b['runs']), "Number of runs in input files must be the same"


llm = LLM(
    Namespace(
        inference_mode="api_self_hosted",
        api_key="None",
        base_url="http://localhost:9876/v1/",
        llm_in_use="Qwen/Qwen2.5-72B-Instruct",
        fast_mode=False,
        max_retry=3,
        max_tokens=16384,
        num_workers=1,
        max_batch_size=16,
    )
)

llm.initialize()

remove_special_tags = lambda x: x.replace("<think>None</think>","").replace("<think>", "\n").replace("</think>", "\n").replace("<solution>", "\n").replace("</solution>", "\n").strip()

all_annotations = []
for run_i, (run_a, run_b) in enumerate(zip(data_a['runs'], data_b['runs'])):
    runs_prompts = []
    
    print(f"Processing run {run_i} of {len(data_a['runs'])}")
    for result_a, result_b in tqdm(zip(run_a['results'], run_b['results']), total=len(run_a['results'])):
        response_a = remove_special_tags(result_a['response'])
        response_b = remove_special_tags(result_b['response'])
        num_tokens_a = len(tokenizer.encode(response_a))
        num_tokens_b = len(tokenizer.encode(response_b))
        runs_prompts.append([{
            "role": "system",
            "content": SYSTEM_PROMPT
        }, {
            "role": "user",
            "content": USER_PROMPT.format(response_a=response_a, num_tokens_a=num_tokens_a, response_b=response_b, num_tokens_b=num_tokens_b)
        }])
    runs_annotations = []
    llm_responses = llm.generate(runs_prompts)
    for i,response in enumerate(llm_responses['responses']):
        if type(response) == dict:
            solution = response.get("solution", "")
            reasoning = response.get("reasoning", "")
            response = f"<think>{reasoning}</think><solution>{solution}</solution>"
        else:
            solution = response
            reasoning = ""
        parsed_response = validate_and_parse_json_output(response)
        if parsed_response is None:
            print(f"Invalid JSON output for sample {i}: {response}")
            continue
        runs_annotations.append(parsed_response)
    all_annotations.append(runs_annotations)

    # Save the annotations multiple times accumulatively
    with open(output_file, "w") as f:
        json.dump(all_annotations, f, indent=4)
    print(f"Responses saved to {output_file} for run {run_i}")
