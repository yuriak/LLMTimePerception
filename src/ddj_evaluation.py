import datasets
from datasets import load_from_disk, Dataset
from transformers import AutoTokenizer
from llm import LLM
import argparse
import tiktoken
import random
import math
from datetime import datetime, timedelta
import os
from utils import extract_json_from_text, validate_and_parse_json_output
import json

tokenizer = tiktoken.encoding_for_model("gpt-4o")


def count_tokens(text):
    """
    Count the number of tokens in a given text using the GPT-4o tokenizer.
    """
    tokens = tokenizer.encode(text)
    return len(tokens)


def add_args(parser):
    parser.add_argument(
        "--model_name",
        type=str,
        default="meta-llama/Llama-3.1-8B-Instruct",
        help="Name of the model to use for evaluation.",
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="./data/lmsys_pair_chat_subset/",
        help="Path to the dataset for evaluation.",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./output",
        help="Directory to save the evaluation results.",
    )
    parser.add_argument(
        "--task_types",
        type=str,
        default="all",
        help="Task types to evaluate. Options: all, or specific task_dataset names",
    )
    parser.add_argument(
        "--save_dataset",
        action="store_true",
        help="Whether to save the dataset after processing.",
    )
    parser.add_argument(
        "--num_runs",
        type=int,
        default=1,
        help="Number of runs for the evaluation.",
    )

    # return parser.parse_args()


SYS_PROMPT = """
You are a helpful assistant.
"""

PROMPT_TYPE1 = """
## Conversation A

### Prompt: 

{prompt_1}

### Response: 

{reply_1}

---

## Conversation B

### Prompt: 

{prompt_2}

### Response: 

{reply_2}

---

You are given two conversations between a user and an LLM. Each conversation consists of a prompt and a response. The conversations are labeled as A and B.

Based on these information, please answer the following question:
**Which response takes the LLM longer to generate? (A or B)**

Assuming that both responses are generated by the same LLM, the inference speed of the LLM is constant and identical for both responses.

Please answer with a JSON object with the following keys:
- `reason`: a brief justification of your choice.
- `answer`: either "A" or "B".

Except for the JSON object, please do not add any other text such as explanations or comments in your answer.
"""


PROMPT_TYPE1_EASY = """
## Conversation A

### Prompt: 

{prompt_1}

### Response: 

{reply_1}

---

## Conversation B

### Prompt: 

{prompt_2}

### Response: 

{reply_2}

---

You are given two conversations between a user and an LLM. Each conversation consists of a prompt and a response. The conversations are labeled as A and B.

Based on these information, please answer the following question:
**Which response takes the LLM longer to generate? (A or B)**

Assuming that both responses are generated by the same LLM, the inference speed of the LLM is constant and identical for both responses, the time taken to complete a response is directly proportional to the number of tokens in the response.

Please answer with a JSON object with the following keys:
- `reason`: a brief justification of your choice.
- `answer`: either "A" or "B".

Except for the JSON object, please do not add any other text such as explanations or comments in your answer.
"""


PROMPT_TYPE1_VERY_EASY = """
## Conversation A

### Prompt: 

{prompt_1}

### Response: 

{reply_1}

### Number of tokens for the dialogue
{tlen_a}

---

## Conversation B

### Prompt: 

{prompt_2}

### Response: 

{reply_2}

### Number of tokens for the dialogue
{tlen_b}

---

You are given two conversations between a user and an LLM. Each conversation consists of a prompt, a response and the number of tokens for the dialogue. The conversations are labeled as A and B.

Based on these information, please answer the following question:
**Which response takes the LLM longer to generate? (A or B)**

Assuming that both responses are generated by the same LLM, the inference speed of the LLM is constant and identical for both responses, the time taken to complete a response is directly proportional to the number of tokens in the response.

Please answer with a JSON object with the following keys:
- `reason`: a brief justification of your choice.
- `answer`: either "A" or "B".

Except for the JSON object, please do not add any other text such as explanations or comments in your answer.
"""


PROMPT_TYPE2 = """
## Conversation A

### Prompt: 


[LOG: {p1_start} - User input started]

{prompt_1}

[LOG: {p1_end} - User input completed]

### Response: 

[LOG: {r1_start} - LLM output started]

{reply_1}

[LOG: {r1_end} - LLM output completed]

---

## Conversation B

### Prompt: 

[LOG: {p2_start} - User input started]

{prompt_2}

[LOG: {p2_end} - User input completed]

### Response: 

[LOG: {r2_start} - LLM output started]

{reply_2}

[LOG: {r2_end} - LLM output completed]

---


You are given two conversations between a user and an LLM. Each conversation consists of a prompt and a response. The conversations are labeled as A and B.
Except for the content of the conversations, all timestamps are also provided, indicating when the user started typing, when the LLM started generating the response, and when the response was completed.

Based on these information, please answer the following question:
**Which response takes the LLM longer to generate? (A or B)*

Please answer with a JSON object with the following keys:
- `reason`: a brief justification of your choice.
- `answer`: either "A" or "B".

Except for the JSON object, please do not add any other text such as explanations or comments in your answer.
"""


PROMPT_TYPE2_MISLEADING_TOKEN = """
## Conversation A

### Prompt: 

[LOG: {p1_start} - User input started]

{prompt_1}

[LOG: {p1_end} - User input completed]

### Response: 

[LOG: {r1_start} - LLM output started]

{reply_1}

[LOG: {r1_end} - LLM output completed]

[LOG: {tlen_a} tokens consumed for the dialogue]

---

## Conversation B

### Prompt: 

[LOG: {p2_start} - User input started]

{prompt_2}

[LOG: {p2_end} - User input completed]

### Response: 

[LOG: {r2_start} - LLM output started]

{reply_2}

[LOG: {r2_end} - LLM output completed]

[LOG: {tlen_b} tokens consumed for the dialogue]

---


You are given two conversations between a user and an LLM. Each conversation consists of a prompt and a response. The conversations are labeled as A and B.
Except for the content of the conversations, all timestamps are also provided, indicating when the user started typing, when the LLM started generating the response, and when the response was completed.
At the same time, the number of tokens consumed for each conversation is also provided.

Based on these information, please answer the following question:
**Which response takes the LLM longer to generate? (A or B)*

Please answer with a JSON object with the following keys:
- `reason`: a brief justification of your choice.
- `answer`: either "A" or "B".

Except for the JSON object, please do not add any other text such as explanations or comments in your answer.
"""


def create_type1_samples(dataset: Dataset, easy_level: int = 0):
    def create_one_sample(item):
        len_a = item["len_a"]
        len_b = item["len_b"]
        prompt_1 = item["conversation_a"][0]["content"]
        reply_1 = item["conversation_a"][1]["content"]
        prompt_2 = item["conversation_b"][0]["content"]
        reply_2 = item["conversation_b"][1]["content"]

        len_a = count_tokens(prompt_1) + count_tokens(reply_1)
        len_b = count_tokens(prompt_2) + count_tokens(reply_2)

        reverse = random.choice([True, False])
        if reverse:
            prompt_1, reply_1, prompt_2, reply_2 = (
                prompt_2,
                reply_2,
                prompt_1,
                reply_1,
            )
            len_a, len_b = len_b, len_a
        if len_a > len_b:
            label = "A"
        else:
            label = "B"

        sample = {
            "prompt_1": prompt_1,
            "reply_1": reply_1,
            "prompt_2": prompt_2,
            "reply_2": reply_2,
            "label": label,
            "tlen_a": len_a,
            "tlen_b": len_b,
            "reverse": reverse,
        }
        return sample

    dataset = dataset.map(
        create_one_sample,
        remove_columns=dataset.column_names,
    ).filter(
        lambda x: x["prompt_1"] is not None and x["reply_1"] is not None,
    )
    print(f"After filtering, the dataset contains {len(dataset)} samples.")
    prompt_template = {
        0: PROMPT_TYPE1,
        1: PROMPT_TYPE1_EASY,
        2: PROMPT_TYPE1_VERY_EASY,
    }.get(easy_level, PROMPT_TYPE1)
    dataset = dataset.map(
        lambda x: {
            "messages": [
                {"role": "system", "content": SYS_PROMPT.strip()},
                {"role": "user", "content": prompt_template.format(**x).strip()},
            ],
            "reverse": x["reverse"],
            "label": x["label"],
        },
        remove_columns=dataset.column_names,
    )
    return dataset


def create_type2_samples(dataset: Dataset, misleading: int = 0):
    """Add timestamp signals to each sample.
    When `misleading` is True, the durations are counter-intuitive:
    the shorter conversation gets the longer elapsed time."""

    # Helper ─ random start‐of‐day between 2023-01-01 and 2024-12-31
    def random_datetime():
        base = datetime(2023, 1, 1).timestamp()
        span = (datetime(2024, 12, 31) - datetime(2023, 1, 1)).total_seconds()
        return datetime.fromtimestamp(base + random.random() * span)

    # Helper ─ convert to required string format
    fmt = "%Y-%m-%d %H-%M-%S"
    to_str = lambda dt: dt.strftime(fmt)

    def create_one_sample(item):
        # ---------- pick a turn ----------
        prompt_1 = item["conversation_a"][0]["content"]
        reply_1 = item["conversation_a"][1]["content"]
        prompt_2 = item["conversation_b"][0]["content"]
        reply_2 = item["conversation_b"][1]["content"]

        # ---------- compute token lengths ----------
        # len_a = item["len_a"]  # prompt+reply tokens
        # len_b = item["len_b"]
        pa_len = count_tokens(prompt_1)
        ra_len = count_tokens(reply_1)
        pb_len = count_tokens(prompt_2)
        rb_len = count_tokens(reply_2)

        # ---------- optional swap so “A/B” isn’t position‐biased ----------
        reverse = random.choice([True, False])
        if reverse:
            prompt_1, prompt_2 = prompt_2, prompt_1
            reply_1, reply_2 = reply_2, reply_1
            pa_len, pb_len = pb_len, pa_len
            ra_len, rb_len = rb_len, ra_len

        # ---------- map length → duration ----------
        # seconds-per-token gives realistic but readable gaps (≈0.6–1.2 s)
        sec_per_tok = random.uniform(0.5, 1.2)
        dur_pa = pa_len * sec_per_tok
        dur_ra = ra_len * sec_per_tok
        dur_pb = pb_len * sec_per_tok
        dur_rb = rb_len * sec_per_tok

        # ---------- invert for the misleading condition ----------
        if misleading > 0:
            dur_pa, dur_pb = dur_pb, dur_pa
            dur_ra, dur_rb = dur_rb, dur_ra

        # ---------- build the eight timestamps ----------
        # We treat each conversation as:
        #   user typing 30 % of total duration  → prompt
        #   short system latency (1–3 s)
        #   LLM responding 40 % of total duration → reply
        latency = lambda: timedelta(seconds=random.uniform(0.5, 1.5))

        def make_ts(base_dt, p_dur, r_dur):
            p_start = base_dt
            p_end = p_start + timedelta(seconds=p_dur)
            r_start = p_end + latency()
            r_end = r_start + timedelta(seconds=r_dur)
            return map(to_str, (p_start, p_end, r_start, r_end))

        # Make sure the two conversations start at different times so that
        # models can’t exploit absolute time—only elapsed time.
        base_a = random_datetime()
        base_b = random_datetime()

        p1_start, p1_end, r1_start, r1_end = make_ts(base_a, dur_pa, dur_ra)
        p2_start, p2_end, r2_start, r2_end = make_ts(base_b, dur_pb, dur_rb)

        # ---------- ground-truth label (who really took longer) ----------
        label = "A" if dur_ra > dur_rb else "B"

        return {
            "prompt_1": prompt_1,
            "reply_1": reply_1,
            "prompt_2": prompt_2,
            "reply_2": reply_2,
            "p1_start": p1_start,
            "p1_end": p1_end,
            "r1_start": r1_start,
            "r1_end": r1_end,
            "p2_start": p2_start,
            "p2_end": p2_end,
            "r2_start": r2_start,
            "r2_end": r2_end,
            "label": label,
            "reverse": reverse,
            "tlen_a": pa_len + ra_len,
            "tlen_b": pb_len + rb_len,
        }

    dataset = dataset.map(
        create_one_sample, remove_columns=dataset.column_names
    ).filter(
        lambda x: x["label"] is not None
    )  # keep only successful ones
    print(f"After filtering, the dataset contains {len(dataset)} samples.")

    prompt_template = {
        0: PROMPT_TYPE2,
        1: PROMPT_TYPE2,
        2: PROMPT_TYPE2_MISLEADING_TOKEN,
    }.get(misleading, PROMPT_TYPE2)

    dataset = dataset.map(
        lambda x: {
            "messages": [
                {"role": "system", "content": SYS_PROMPT.strip()},
                {"role": "user", "content": prompt_template.format(**x).strip()},
            ],
            "reverse": x["reverse"],
            "label": x["label"],
        },
        remove_columns=dataset.column_names,
    )
    return dataset


def hard_match(response):
    for k in [
        '"answer": "A"',
        "'answer': 'A'",
        '"answer":"A"',
        "'answer':'A'",
        '"answer": "A"',
        '"answer":"A"',
    ]:
        if k in response:
            return "A"
    for k in [
        '"answer": "B"',
        "'answer': 'B'",
        '"answer":"B"',
        "'answer':'B'",
        '"answer": "B"',
        '"answer":"B"',
    ]:
        if k in response:
            return "B"
    return ""


def evaluate_dataset(llm: LLM, dataset: Dataset, runs: int = 1):
    final_results = {"runs": []}
    for n in range(runs):
        print(f"Run {n + 1}/{runs}...")
        messages = dataset["messages"]
        labels = dataset["label"]
        reverses = dataset["reverse"]
        results = []

        responses = llm.generate(messages)["responses"]
        for i, response in enumerate(responses):
            try:
                if type(response) == dict:
                    solution = response.get("solution", "")
                    reasoning = response.get("reasoning", "")
                    response = (
                        f"<think>{reasoning}</think><solution>{solution}</solution>"
                    )
                else:
                    solution = response
                    reasoning = ""
                parsed_response = validate_and_parse_json_output(
                    response, remove_escape=True
                )
                if parsed_response is None:
                    answer = hard_match(response)
                    if answer == "":
                        print(f"Invalid JSON output for sample {i}: {response}")
                else:
                    answer = str(parsed_response.get("answer", ""))
                result = {
                    "run": n,
                    "messages": messages[i],
                    "response": response,
                    "parsed_response": parsed_response,
                    "answer": answer,
                    "label": labels[i],
                    "reverse": reverses[i],
                    "correct": answer == labels[i],
                }
                results.append(result)
            except Exception as e:
                print(f"Error processing sample {i}: {e}")
        # compute accuracy
        correct_count = sum(1 for result in results if result["correct"])
        total_count = len(results)
        accuracy = correct_count / total_count if total_count > 0 else 0
        run_results = {
            "accuracy": accuracy,
            "total_count": total_count,
            "correct_count": correct_count,
            "results": results,
        }
        final_results["runs"].append(run_results)
    return final_results


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate time passage awareness of LLMs on QA tasks."
    )
    add_args(parser)
    LLM.add_arguments(parser)
    args = parser.parse_args()

    model_name = args.llm_in_use
    dataset_path = args.dataset_path
    output_dir = args.output_dir
    # Load the dataset
    raw_dataset = load_from_disk(dataset_path)
    print(f"Loaded dataset from {dataset_path}.")
    # Check output dir and see if all datasets exist
    output_dir = f"{output_dir}/{model_name.replace('/', '_')}"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    required_datasets = [
        "type1_dataset",
        "type1_easy_dataset",
        "type1_very_easy_dataset",
        "type2_dataset",
        "type2_misleading_dataset",
        "type2_misleading_with_token_dataset",
    ]
    dataset_builder = {
        "type1_dataset": (create_type1_samples, {"easy_level": 0}),
        "type1_easy_dataset": (create_type1_samples, {"easy_level": 1}),
        "type1_very_easy_dataset": (create_type1_samples, {"easy_level": 2}),
        "type2_dataset": (create_type2_samples, {"misleading": 0}),
        "type2_misleading_dataset": (create_type2_samples, {"misleading": 1}),
        "type2_misleading_with_token_dataset": (
            create_type2_samples,
            {"misleading": 2},
        ),
    }

    available_datasets = {}
    for dataset_name in required_datasets:
        dataset_path = os.path.join(output_dir, dataset_name)
        if os.path.exists(dataset_path):
            print(f"{dataset_name} already exists at {output_dir}/{dataset_name}.")
            available_datasets[dataset_name] = load_from_disk(dataset_path)
        else:
            print(f"{dataset_name} does not exist at {output_dir}/{dataset_name}.")
            dataset_builder_func, kwargs = dataset_builder[dataset_name]
            task_dataset = dataset_builder_func(raw_dataset, **kwargs)
            print(f"Created {dataset_name} with {len(task_dataset)} samples.")
            if args.save_dataset:
                task_dataset.save_to_disk(dataset_path)
            available_datasets[dataset_name] = task_dataset
    # Check if all datasets are available
    for dataset_name in required_datasets:
        if dataset_name not in available_datasets:
            print(f"{dataset_name} is missing. Exiting.")
            return

    # Load the model
    llm = LLM(args)
    llm.initialize()

    task_types = args.task_types

    task_dataset_candidates = []
    if task_types == "all":
        task_dataset_candidates = [
            "type1_dataset",
            "type1_easy_dataset",
            "type1_very_easy_dataset",
            "type2_dataset",
            "type2_misleading_dataset",
            "type2_misleading_with_token_dataset",
        ]
    else:
        task_dataset_candidates = task_types.split(",")

    prepared_datasets = {}
    for dataset_name in task_dataset_candidates:
        if dataset_name in available_datasets:
            prepared_datasets[dataset_name] = available_datasets[dataset_name]
        else:
            print(f"{dataset_name} is not available. Skipping.")
            continue
    # Evaluate the model on the selected datasets
    for dataset_name, dataset in prepared_datasets.items():
        print(f"Evaluating {dataset_name}...")
        results = evaluate_dataset(llm, dataset, args.num_runs)
        with open(f"{output_dir}/{dataset_name}_results.json", "w") as f:
            json.dump(results, f, indent=4)
        # Compute the average accuracy
        avg_accuracy = sum(run["accuracy"] for run in results["runs"]) / len(
            results["runs"]
        )
        print(f"{dataset_name} evaluation results: {avg_accuracy:.2%} accuracy")

    print("Evaluation completed.")
    llm.unload()


if __name__ == "__main__":
    main()
